{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version 1.2.0\n",
      "PIL version 6.1.0\n",
      "Device cuda:2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets , transforms\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import PIL \n",
    "from PIL import Image\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from pathlib import Path\n",
    "device = (\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"torch version {torch.__version__}\\nPIL version {PIL.__version__}\\nDevice {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Mean is [125.30691805 122.95039414 113.86538318]\n",
      "Std dev is  [62.99321928 62.08870764 66.70489964]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.CIFAR10(train=True,root=\"data/\" , download=True)\n",
    "mean = np.mean(dataset.data ,axis=(0,1,2))\n",
    "std = np.std(dataset.data ,axis=(0,1,2))\n",
    "print(f\"\\nMean is {mean}\\nStd dev is  {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomCrop(size=32 , padding=4 , padding_mode=\"symmetric\",pad_if_needed=True),\n",
    "                                       transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       #transforms.Normalize(mean = mean , std=std)\n",
    "                                      ])\n",
    "\n",
    "val_transforms = transforms.Compose([transforms.ToTensor() , \n",
    "                                     #transforms.Normalize(mean=mean , std=std)\n",
    "                                    ])\n",
    "\n",
    "mean = torch.from_numpy(mean).type(torch.float32)\n",
    "std = torch.from_numpy(std).type(torch.float32)\n",
    "\n",
    "def denormalize(image):\n",
    "  image = image.clone().detach().to(\"cpu\") # take out of computational graph\n",
    "  image = image.squeeze() \n",
    "  image = image.permute(1,2,0) # channel swapping H*W*C\n",
    " # image.mul_(std).add_(mean) # denormalize\n",
    "\n",
    "  return image.numpy() \n",
    "\n",
    "trainset = datasets.CIFAR10(train=True,root=\"data/\" , download=True,transform=train_transforms)\n",
    "valset  = datasets.CIFAR10(train=False,root=\"data/\" , download=True,transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self,num_channels,growth_rate,bn_size,drop_rate):\n",
    "        super(DenseLayer,self).__init__()\n",
    "        mid_channel = int(growth_rate*bn_size)\n",
    "        self.add_module(\"bn1\",nn.BatchNorm2d(num_channels))\n",
    "        self.add_module(\"relu1\",nn.ReLU(inplace=True))\n",
    "        self.add_module(\"conv1\",nn.Conv2d(num_channels,mid_channel ,kernel_size=1 , bias=False))\n",
    "        self.add_module(\"bn2\",nn.BatchNorm2d(mid_channel))\n",
    "        self.add_module(\"relu2\",nn.ReLU(inplace=True))\n",
    "        self.add_module(\"conv2\",nn.Conv2d(mid_channel ,growth_rate,kernel_size=3,padding=1 , bias=False))\n",
    "        self.drop_rate=drop_rate\n",
    "    def forward(self,*prev_features):\n",
    "        concated_features = torch.cat(prev_features, 1)\n",
    "        bottleneck_output = self.conv1(self.relu1(self.bn1(concated_features)))\n",
    "        new_features = self.conv2(self.relu2(self.bn2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return new_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(nn.Module):\n",
    "    def __init__(self,num_channels,num_out_channels):\n",
    "        super(Transition,self).__init__()\n",
    "        self.add_module(\"bn\",nn.BatchNorm2d(num_channels))\n",
    "        self.add_module(\"relu\",nn.ReLU(inplace=True))\n",
    "        self.add_module(\"conv\",nn.Conv2d(num_channels,num_out_channels ,kernel_size=1 , bias=False))\n",
    "        self.add_module(\"pool\",nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "    def forward(self,x):\n",
    "        out = self.conv(self.relu(self.bn(x)))\n",
    "        out = self.pool(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_layers,num_channels,growth_rate,bn_size,drop_rate):\n",
    "        super(DenseBlock,self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = DenseLayer(num_channels=num_channels+i*growth_rate,\n",
    "                               growth_rate=growth_rate,\n",
    "                               bn_size=bn_size,\n",
    "                               drop_rate=drop_rate)\n",
    "            self.add_module(f\"denselayer{i+1}\",layer)\n",
    "    \n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.named_children():\n",
    "            new_features = layer(*features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,growth_rate=32,block_config=(6,12,24,16),\n",
    "                num_init_features=64,bn_size=4, drop_rate=0.1,num_classes=dataset.classes.__len__()):\n",
    "        super(DenseNet,self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            (\"conv0\",nn.Conv2d(3,num_init_features,kernel_size=3,bias=False)),\n",
    "        ]))\n",
    "        \n",
    "        num_features=num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block  = DenseBlock(num_layers = num_layers,\n",
    "                               num_channels=num_features,\n",
    "                                growth_rate=growth_rate,\n",
    "                                bn_size=bn_size,\n",
    "                                drop_rate=drop_rate)\n",
    "            self.features.add_module(f\"denseblock{i+1}\",block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i<len(block_config)-1:\n",
    "                transition = Transition(num_features,num_features//2)\n",
    "                num_features=num_features//2\n",
    "                self.features.add_module(f\"transition{i+1}\",transition)\n",
    "        self.features.add_module(\"norm5\",nn.BatchNorm2d(num_features))\n",
    "        self.classifier = nn.Linear(num_features,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64*torch.cuda.device_count()\n",
    "epochs = 10  # number of epochs to run without early-stopping\n",
    "workers = 4  # number of workers for loading data in the DataLoader\n",
    "lr = 1e-3  # learning rate\n",
    "weight_decay = 1e-4  # weight decay\n",
    "n_classes = len(trainset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_param = { \"batch_size\":batch_size,\n",
    "                 \"pin_memory\":True,\n",
    "                 \"num_workers\":workers,\n",
    "                \"shuffle\":True}\n",
    "\n",
    "trainLoader = DataLoader(trainset,**loader_param)\n",
    "\n",
    "valLoader = DataLoader(valset  ,**loader_param)\n",
    "\n",
    "data_loader={\"train\":trainLoader , \"val\":valLoader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model = DenseNet()\n",
    "model_super_conv = copy.deepcopy(model)\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_super_conv = torch.nn.DataParallel(model_super_conv).to(device)\n",
    "optimizer_super_conv = optim.Adam(model_super_conv.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def train(model , data_loader , criterion , optimizer , num_epochs=5):\n",
    "\n",
    "  for epoch in trange(num_epochs,desc=\"Epochs\"):\n",
    "    result = []\n",
    "    for phase in ['train', 'val']:\n",
    "      if phase==\"train\":     # put the model in training mode\n",
    "        model.train()\n",
    "      else:     # put the model in validation mode\n",
    "        model.eval()\n",
    "       \n",
    "      # keep track of training and validation loss\n",
    "      running_loss = 0.0\n",
    "      running_corrects = 0.0  \n",
    "      \n",
    "      for data , target in data_loader[phase]:\n",
    "        #load the data and target to respective device\n",
    "        data , target = data.to(device)  , target.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(phase==\"train\"):\n",
    "          #feed the input\n",
    "          output = model(data)\n",
    "          #calculate the loss\n",
    "          loss = criterion(output,target)\n",
    "          preds = torch.argmax(output,1)\n",
    "\n",
    "          if phase==\"train\"  :\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters \n",
    "            loss.backward()\n",
    "            # update the model parameters\n",
    "            optimizer.step()\n",
    "            # zero the grad to stop it from accumulating\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "        running_corrects += torch.sum(preds == target.data).item()\n",
    "        \n",
    "        \n",
    "      epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "      epoch_acc = running_corrects / len(data_loader[phase].dataset)\n",
    "\n",
    "      result.append('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 1/10 [01:28<13:17, 88.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 1.4295 Acc: 0.4769', 'val Loss: 1.3306 Acc: 0.5422']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:  20%|██        | 2/10 [02:40<11:09, 83.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.9521 Acc: 0.6616', 'val Loss: 0.9158 Acc: 0.6860']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:  30%|███       | 3/10 [03:52<09:20, 80.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.7254 Acc: 0.7466', 'val Loss: 0.8087 Acc: 0.7457']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:  40%|████      | 4/10 [05:05<07:47, 77.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.6033 Acc: 0.7917', 'val Loss: 0.7423 Acc: 0.7456']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:  50%|█████     | 5/10 [06:16<06:19, 75.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.5112 Acc: 0.8235', 'val Loss: 0.5424 Acc: 0.8178']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:  60%|██████    | 6/10 [07:28<04:59, 74.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.4433 Acc: 0.8471', 'val Loss: 0.6686 Acc: 0.7810']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:  70%|███████   | 7/10 [08:41<03:42, 74.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.4110 Acc: 0.8575', 'val Loss: 0.4973 Acc: 0.8311']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:  80%|████████  | 8/10 [09:53<02:27, 73.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.3714 Acc: 0.8727', 'val Loss: 0.3734 Acc: 0.8678']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs:  90%|█████████ | 9/10 [11:05<01:13, 73.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.3342 Acc: 0.8841', 'val Loss: 0.4605 Acc: 0.8484']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epochs: 100%|██████████| 10/10 [12:18<00:00, 72.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.3100 Acc: 0.8921', 'val Loss: 0.5318 Acc: 0.8355']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model,data_loader , criterion, optimizer,num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stepper():\n",
    "    \"Used to \\\"step\\\" from start,end (`vals`) over `n_iter` iterations on a schedule defined by `func`\"\n",
    "    \n",
    "    def __init__(self, val, n_iter:int, func):\n",
    "        self.start,self.end = val\n",
    "        self.n_iter = max(1,n_iter)\n",
    "        self.func = func\n",
    "        self.n = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"Return next value along annealed schedule.\"\n",
    "        self.n += 1\n",
    "        return self.func(self.start, self.end, self.n/self.n_iter)\n",
    "\n",
    "    @property\n",
    "    def is_done(self):\n",
    "        \"Return `True` if schedule completed.\"\n",
    "        return self.n >= self.n_iter\n",
    "    \n",
    "# Annealing functions\n",
    "def annealing_no(start, end, pct):\n",
    "    \"No annealing, always return `start`.\"\n",
    "    return start\n",
    "  \n",
    "def annealing_linear(start, end, pct):\n",
    "    \"Linearly anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "    return start + pct * (end-start)\n",
    "  \n",
    "def annealing_exp(start, end, pct):\n",
    "    \"Exponentially anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "    return start * (end/start) ** pct\n",
    "\n",
    "def annealing_cos(start, end, pct):\n",
    "    \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "    cos_out = np.cos(np.pi * pct) + 1\n",
    "    return end + (start-end)/2 * cos_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OneCyclePolicy:\n",
    "  \n",
    "  def __init__(self,model , optimizer , criterion ,num_iteration,num_epochs,max_lr, momentum = (0.95,0.85) , div_factor=25 , pct_start=0.4, device=None ):\n",
    "    \n",
    "    self.model =model\n",
    "    self.optimizer = optimizer\n",
    "    self.criterion = criterion\n",
    "    self.num_epochs = num_epochs\n",
    "    if device is None:\n",
    "      self.device = next(model.parameters()).device\n",
    "    else:\n",
    "      self.device = device\n",
    "      \n",
    "    n = num_iteration * self.num_epochs\n",
    "    a1 = int(n*pct_start)\n",
    "    a2 = n-a1\n",
    "    self.phases = ((a1 , annealing_linear) , (a2 , annealing_cos))\n",
    "    min_lr = max_lr/div_factor\n",
    "    self.lr_scheds = self.steps((min_lr,max_lr) , (max_lr,min_lr/1e4))\n",
    "    self.mom_scheds =self.steps(momentum , momentum[::-1])\n",
    "    self.idx_s = 0\n",
    "    self.update_lr_mom(self.lr_scheds[0].start,self.mom_scheds[0].start)\n",
    "  \n",
    "  def steps(self, *steps):\n",
    "      \"Build anneal schedule for all of the parameters.\"\n",
    "      return [Stepper(step, n_iter, func=func)for (step,(n_iter,func)) in zip(steps, self.phases)]\n",
    "\n",
    "  def train(self, data_loader ):\n",
    "    self.model.to(self.device)\n",
    "#     data_loader = {\"train\":trainLoader , \"val\":validLoader}\n",
    "    for epoch in tqdm(range(self.num_epochs),desc=\"Epochs\"):\n",
    "      result = []\n",
    "      for phase in ['train', 'val']:\n",
    "        if phase==\"train\":     # put the model in training mode\n",
    "          model.train()\n",
    "        else:     # put the model in validation mode\n",
    "          model.eval()\n",
    "\n",
    "        # keep track of training and validation loss\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0  \n",
    "\n",
    "        for data , target in data_loader[phase]:\n",
    "          #load the data and target to respective device\n",
    "          data , target = data.to(device)  , target.to(device)\n",
    "\n",
    "          with torch.set_grad_enabled(phase==\"train\"):\n",
    "            #feed the input\n",
    "            output = self.model(data)\n",
    "            #calculate the loss\n",
    "            loss = self.criterion(output,target)\n",
    "            preds = torch.argmax(output,1)\n",
    "\n",
    "            if phase==\"train\"  :\n",
    "              # backward pass: compute gradient of the loss with respect to model parameters \n",
    "              loss.backward()\n",
    "              # update the model parameters\n",
    "              self.optimizer.step()\n",
    "              # zero the grad to stop it from accumulating\n",
    "              self.optimizer.zero_grad()\n",
    "            \n",
    "              self.update_lr_mom(self.lr_scheds[self.idx_s].step() ,self.mom_scheds[self.idx_s].step() )\n",
    "\n",
    "              if self.lr_scheds[self.idx_s].is_done:\n",
    "                self.idx_s += 1\n",
    "          \n",
    "          # statistics\n",
    "          running_loss += loss.item() * data.size(0)\n",
    "          running_corrects += torch.sum(preds == target.data).item()\n",
    "\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "        epoch_acc = running_corrects/ len(data_loader[phase].dataset)\n",
    "\n",
    "        result.append('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "      print(result)\n",
    "\n",
    "  def update_lr_mom(self,lr=0.001,mom=0.99):\n",
    "    for l in self.optimizer.param_groups:\n",
    "      l[\"lr\"]=lr\n",
    "      if isinstance(self.optimizer , ( torch.optim.Adamax,torch.optim.Adam)):\n",
    "          l[\"betas\"] = ( mom, 0.999)\n",
    "      elif isinstance(self.optimizer, torch.optim.SGD):\n",
    "          l[\"momentum\"] =mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  10%|█         | 1/10 [01:11<10:44, 71.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 1.4461 Acc: 0.4697', 'val Loss: 1.0721 Acc: 0.6178']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs:  20%|██        | 2/10 [02:25<09:38, 72.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 1.0015 Acc: 0.6448', 'val Loss: 0.8674 Acc: 0.6971']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs:  30%|███       | 3/10 [03:39<08:28, 72.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.8092 Acc: 0.7162', 'val Loss: 0.6769 Acc: 0.7651']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs:  40%|████      | 4/10 [04:52<07:17, 72.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.6752 Acc: 0.7659', 'val Loss: 0.5890 Acc: 0.7985']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs:  50%|█████     | 5/10 [06:06<06:06, 73.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.5759 Acc: 0.8013', 'val Loss: 0.4973 Acc: 0.8312']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs:  60%|██████    | 6/10 [07:20<04:53, 73.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.4715 Acc: 0.8388', 'val Loss: 0.4445 Acc: 0.8460']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs:  70%|███████   | 7/10 [08:33<03:40, 73.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.3879 Acc: 0.8655', 'val Loss: 0.3772 Acc: 0.8733']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs:  80%|████████  | 8/10 [09:46<02:26, 73.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.3149 Acc: 0.8902', 'val Loss: 0.3235 Acc: 0.8907']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs:  90%|█████████ | 9/10 [11:00<01:13, 73.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.2467 Acc: 0.9131', 'val Loss: 0.2872 Acc: 0.9028']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs: 100%|██████████| 10/10 [12:14<00:00, 73.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train Loss: 0.2066 Acc: 0.9283', 'val Loss: 0.2844 Acc: 0.9036']\n"
     ]
    }
   ],
   "source": [
    "fit_one_cycle = OneCyclePolicy(model_super_conv ,optimizer_super_conv , criterion,num_iteration=len(trainLoader)  , num_epochs =10 , max_lr =0.01 ,device=device)\n",
    "fit_one_cycle.train(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
