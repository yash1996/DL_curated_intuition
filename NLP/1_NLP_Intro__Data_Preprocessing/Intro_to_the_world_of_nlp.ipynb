{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't worry if you don't understand everything at first! You're not supposed to. We will start using some \"black boxes\" and then we'll dig into the lower level details later.\n",
    "\n",
    "## To start, focus on what things DO, not what they ARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP?\n",
    "    \n",
    "    Natural Language Processing is technique where computers try an understand human language and make meaning out of it.\n",
    "    \n",
    "\n",
    "NLP is a broad field, encompassing a variety of tasks, including:\n",
    "\n",
    "    1.  Part-of-speech tagging: identify if each word is a noun, verb, adjective, etc.)\n",
    "    2.  Named entity recognition NER): identify person names, organizations, locations, medical codes, time expressions, quantities, monetary values, etc)\n",
    "    3.  Question answering\n",
    "    4.  Speech recognition\n",
    "    5.  Text-to-speech and Speech-to-text\n",
    "    6.  Topic modeling\n",
    "    7.  Sentiment classification\n",
    "    9.  Language modeling\n",
    "    10. Translation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLU?\n",
    " \n",
    " Natural Language Understanding is all about understanding the natural language.\n",
    " \n",
    " Goals of NLU\n",
    "  1. Gain insights into cognition\n",
    "  2. Develop Artifical Intelligent agents as an assistant.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLG?\n",
    "\n",
    "Natural language generation is the natural language processing task of generating natural language from a machine representation system such as a knowledge base or a logical form. \n",
    "\n",
    "Example applications of NLG\n",
    "    1. Recommendation and Comparison \n",
    "    2. Report Generation –Summarization \n",
    "    3. Paraphrase \n",
    "    4. Prompt and response generation in dialogue systems \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "\n",
    "1. [Flair](https://github.com/zalandoresearch/flair)\n",
    "2. [Allen NLP](https://github.com/allenai/allennlp)\n",
    "3. [Deep Pavlov](https://github.com/deepmipt/deeppavlov)\n",
    "4. [Pytext](https://github.com/facebookresearch/PyText)\n",
    "5. [NLTK](https://www.nltk.org/)\n",
    "6. [Hugging Face Pytorch Transformer](https://github.com/huggingface/pytorch-transformers)\n",
    "7. [Spacy](https://spacy.io/)\n",
    "8. [torchtext](https://torchtext.readthedocs.io/en/latest/)\n",
    "9. [Ekphrasis](https://github.com/cbaziotis/ekphrasis)\n",
    "10. [Genism](https://radimrehurek.com/gensim/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "### Sources\n",
    "\n",
    "For Generative Training :- Where the model has to learn about the data and its distribution \n",
    "    1. News Article:- Archives\n",
    "    2. Wikipedia Article \n",
    "    3. Book Corpus \n",
    "    4. Crawling the Internet for webpages.\n",
    "    5. Reddit\n",
    "\n",
    "Generative training on an abundant set of unsupervised data helps in performing Transfer learning for a downstream task where few parameters need to be learnt from sratch and less data is also required.\n",
    "\n",
    "For Determinstic Training :- Where the model learns about Decision boundary within the data.\n",
    "    Generic\n",
    "        1. Kaggle Dataset\n",
    "    Sentiment\n",
    "        1. Product Reviews :- Amazon, Flipkart\n",
    "    Emotion:-\n",
    "        1. ISEAR\n",
    "        2. Twitter dataset\n",
    "    Question Answering:-\n",
    "        1. SQUAD\n",
    "    etc.\n",
    "    \n",
    "### For Vernacular text\n",
    "In vernacular context we have crisis in data especially when it comes to state specific language in India. (Ex. Bengali, Gujurati etc.) \n",
    "Few Sources are:-\n",
    "1. News (Jagran.com, Danik bhaskar)\n",
    "2. Moview reviews (Web Duniya)\n",
    "3. Hindi Wikipedia\n",
    "4. Book Corpus\n",
    "6. IIT Bombay (English-Hindi Parallel Corpus)\n",
    "\n",
    "### Tools\n",
    "1. Scrapy :- Simple, Extensible framework for scraping and crawling websites. Has numerous feature into it.\n",
    "2. Beautiful-Soup :- For Parsing Html and xml documents. \n",
    "3. Excel \n",
    "4. wikiextractor:- A tool for extracting plain text from Wikipedia dumps\n",
    "\n",
    "### Data Annotation Tool\n",
    "\n",
    "1. TagTog\n",
    "2. Prodigy (Explosion AI)\n",
    "3. Mechanical Turk \n",
    "4. PyBossa\n",
    "5. Chakki-works Doccano   \n",
    "6. WebAnno\n",
    "7. Brat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1.  Cleaning\n",
    "2.  Regex \n",
    "    1. Url Cleanup\n",
    "    2. HTML Tag\n",
    "    3. Date\n",
    "    4. Numbers\n",
    "    5. Lingos\n",
    "    6. Emoticons \n",
    "3.  Lemmatization \n",
    "4.  Stemming\n",
    "5.  Chunking\n",
    "6.  POS Tags\n",
    "7.  NER Tags\n",
    "8.  Stopwords\n",
    "9.  Tokenizers\n",
    "10. Spell Correction\n",
    "11. Word Segmentation\n",
    "12. Word Processing \n",
    "    1. Elongated\n",
    "    2. Repeated\n",
    "    3. All Caps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "1. Bag of Words\n",
    "![](https://uc-r.github.io/public/images/analytics/feature-engineering/bow-image.png)\n",
    "2. TF-IDF\n",
    "![](https://miro.medium.com/max/3604/1*ImQJjYGLq2GE4eX40Mh28Q.png)\n",
    "3. Word Embeddings\n",
    "    1. Word2Vec\n",
    "    \n",
    "    Word2Vec is a predictive model.\n",
    "    ![](https://skymind.ai/images/wiki/word2vec_diagrams.png)\n",
    "    2. Glove\n",
    "    \n",
    "    Glove is a Count-based models learn their vectors by essentially doing dimensionality reduction on the co-occurrence counts matrix.\n",
    "    3. FastText\n",
    "        \n",
    "        Fastext is trained in a similar fashion how word2vec model is trained, the only difference is the fastext enchriches the word vectors with subword units.\n",
    "        \n",
    "        [FastText works](https://www.quora.com/What-is-the-main-difference-between-word2vec-and-fastText)\n",
    "        \n",
    "    4. ELMO\n",
    "            \n",
    "           ELMo is a deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). These word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. They can be easily added to existing models and significantly improve the state of the art across a broad range of challenging NLP problems, including question answering, textual entailment and sentiment analysis.\n",
    "           \n",
    "        ELMo representations are:\n",
    "\n",
    "        * Contextual: The representation for each word depends on the entire context in which it is used.\n",
    "        * Deep: The word representations combine all layers of a deep pre-trained neural network.\n",
    "        * Character based: ELMo representations are purely character based, allowing the network to use morphological clues to form robust representations for out-of-vocabulary tokens unseen in training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "1.  RNN\n",
    "![](https://proxy.duckduckgo.com/iu/?u=http%3A%2F%2Fcorochann.com%2Fwp-content%2Fuploads%2F2017%2F05%2Frnn1_expand.png&f=1&nofb=1)\n",
    "\n",
    "RNN suffers from gradient vanishing problem and they do not persist long term dependencies.\n",
    "2.  LSTM\n",
    "\n",
    "Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. \n",
    "\n",
    "LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n",
    "\n",
    "![](https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F1*6vw1g-HNuOgRYPj-IGhddQ.png&f=1&nofb=1)\n",
    "\n",
    "\n",
    "\n",
    "3.  BI-LSTM\n",
    "![](https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn-images-1.medium.com%2Ffreeze%2Fmax%2F1000%2F1*QBrVVvYps5zo6QtBRRq4fA.png%3Fq%3D20&f=1&nofb=1)\n",
    "\n",
    "4.  GRU\n",
    "\n",
    "5.  CNNs\n",
    "6.  Seq-Seq\n",
    "![](https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F1*_6-EVV3RJXD5KDjdnxztzg%402x.png&f=1&nofb=1)\n",
    "\n",
    "7.  Seq-Seq Attention\n",
    "![](https://pravn.files.wordpress.com/2017/11/luong.png?w=319)\n",
    "8.  Pointer Generator Network\n",
    "![](https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Ftse1.mm.bing.net%2Fth%3Fid%3DOIP.c6kke1e2bWMaicGFw7wTwwHaEM%26pid%3DApi&f=1)\n",
    "8.  Transformer\n",
    "![](https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fraw.githubusercontent.com%2FDongjunLee%2Ftransformer-tensorflow%2Fmaster%2Fimages%2Ftransformer-architecture.png&f=1&nofb=1)\n",
    "![](https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s1600/image2.png)\n",
    "9.  GPT\n",
    "![](https://miro.medium.com/max/1772/1*MXspASIUulGBw58PyMA5Ig.png)\n",
    "10. Transformer-XL\n",
    "![](https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.lyrn.ai%2Fwp-content%2Fuploads%2F2019%2F01%2FTransformerXL-featured.png&f=1&nofb=1)\n",
    "11. BERT\n",
    "\n",
    "BERT’s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling.\n",
    "\n",
    "BERT is given billions of sentences at training time. It’s then asked to predict a random selection of missing words from these sentences. After practicing with this corpus of text several times over, BERT adopts a pretty good understanding of how a sentence fits together grammatically. It’s also better at predicting ideas that are likely to show up together.\n",
    "\n",
    "![](https://blog.fastforwardlabs.com/images/2018/12/Screen_Shot_2018_12_07_at_12_03_44_PM-1544202300577.png)\n",
    "![](https://jalammar.github.io/images/bert-tasks.png)\n",
    "12. GPT-2\n",
    "![](https://miro.medium.com/max/1742/1*wUOgqwOJv-eMd0rSjWlTMg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buisness Problem\n",
    "\n",
    "1.  Text Classification\n",
    "     1. Sentiment Classification\n",
    "     2. Emotion Classification\n",
    "     3. Reviews Rating\n",
    "2.  Topic Modeling\n",
    "3.  Named Entity Recognition\n",
    "4.  Part Of Speech Tagging\n",
    "5.  Language Model\n",
    "6.  Machine Translation\n",
    "7.  Question Answering\n",
    "8.  Text Summarization\n",
    "9.  Text Generation\n",
    "10. Image Captioning\n",
    "11. Optical Character Recognition\n",
    "12. Chatbots\n",
    "13. [Dependency Parsing](https://nlpprogress.com/english/dependency_parsing.html)\n",
    "14. [Coreference Resolution](https://en.wikipedia.org/wiki/Coreference) \n",
    "15. [Semantic Textual Similarity](https://nlpprogress.com/english/semantic_textual_similarity.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
