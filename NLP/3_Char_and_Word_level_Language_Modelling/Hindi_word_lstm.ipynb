{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version is 1.2.0 device is cuda\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import torchtext\n",
    "import numpy  as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "from torch import autograd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"torch version is {torch.__version__} device is {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sentence Piece Tokenizer for hindi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your sentence piece model and load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/hindi_updated.txt\") as f:\n",
    "    data = f.readlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_fraction = 0.2\n",
    "val_data = data[len(data)-int(val_fraction * len(data)):]\n",
    "train_data = data[:len(data)-len(val_data)]\n",
    "with open(\"data/hindi_train.txt\" ,\"w\") as t , open(\"data/hindi_val.txt\",\"w\") as v:\n",
    "    t.writelines(train_data)\n",
    "    v.writelines(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataset and Data iterator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(tokenize = sp.EncodeAsPieces,batch_first=True)\n",
    "train_set = torchtext.datasets.LanguageModelingDataset(\"data/hindi_train.txt\",TEXT,newline_eos=False)\n",
    "val_set = torchtext.datasets.LanguageModelingDataset(\"data/hindi_val.txt\",TEXT,newline_eos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_set,val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "train_iter = torchtext.data.BPTTIterator(train_set,batch_size,100,train=True,device=device)\n",
    "val_iter = torchtext.data.BPTTIterator(val_set,batch_size,100,train=False,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 128]\n",
       "\t[.text]:[torch.cuda.LongTensor of size 128x100 (GPU 0)]\n",
       "\t[.target]:[torch.cuda.LongTensor of size 128x100 (GPU 0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Simple Model\n",
    "![](https://i.stack.imgur.com/TtfMs.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HindiLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim , emb_dim, n_hidden=256, n_layers=2,drop_prob=0.25, lr=0.001):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        # Embedding layer\n",
    "        self.embed  = nn.Embedding(input_dim,emb_dim)\n",
    "        # 2 layer lstm\n",
    "        self.lstm = nn.LSTM(emb_dim, n_hidden, n_layers,  dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        #fc\n",
    "        self.fc = nn.Linear(n_hidden, input_dim)\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        x = self.embed(x)    \n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HindiLSTM(\n",
       "  (embed): Embedding(8061, 256)\n",
       "  (lstm): LSTM(256, 300, num_layers=2, batch_first=True, dropout=0.25)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (fc): Linear(in_features=300, out_features=8061, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HindiLSTM(len(TEXT.vocab),256,n_hidden=300).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "lr=0.002\n",
    "temperature=1.0\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "clip=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:24<07:45, 24.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 7.357830519425241 val loss is 7.1380105257034305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:49<07:21, 24.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 6.8942562203658255 val loss is 6.672746467590332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [01:13<06:57, 24.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 6.484894024698358 val loss is 6.375603985786438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [01:37<06:31, 24.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 6.2033439234683385 val loss is 6.206889057159424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [02:02<06:06, 24.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.997840399491159 val loss is 6.091543221473694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [02:26<05:42, 24.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.828500762738679 val loss is 5.986026501655578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [02:51<05:17, 24.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.707831683911775 val loss is 5.929713726043701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [03:15<04:52, 24.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.61639596035606 val loss is 5.873499655723572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [03:39<04:28, 24.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.538553498920641 val loss is 5.838024306297302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [04:04<04:04, 24.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.473059727016248 val loss is 5.8063994407653805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [04:28<03:39, 24.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.413133847086053 val loss is 5.784366488456726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [04:53<03:15, 24.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.3569102588452795 val loss is 5.755792593955993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [05:17<02:50, 24.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.3037426471710205 val loss is 5.729305171966553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [05:41<02:26, 24.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.2547503120020815 val loss is 5.7168717861175535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [06:06<02:01, 24.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.207925412529393 val loss is 5.706015944480896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [06:30<01:37, 24.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.165990161895752 val loss is 5.688183736801148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [06:54<01:13, 24.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.124493847395245 val loss is 5.6801965713500975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [07:19<00:48, 24.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.084050517333181 val loss is 5.666971516609192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [07:43<00:24, 24.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.051133073003668 val loss is 5.6619257688522335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 20/20 [08:08<00:00, 24.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss is 5.017613338169299 val loss is 5.659657549858093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in trange(epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    with autograd.detect_anomaly(): # Only for debugging purpose read the doc\n",
    "        model.train()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        for tr in train_iter:\n",
    "            inputs,targets = tr.text,tr.target\n",
    "            h = tuple([each.data for each in h])\n",
    "            optimizer.zero_grad()\n",
    "            output ,h = model(inputs,h)\n",
    "            loss = criterion(output,targets.view(-1).long())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(),clip )\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        model.eval()    \n",
    "        val_h = model.init_hidden(batch_size)\n",
    "        for vl in val_iter:\n",
    "            inputs,targets = vl.text,vl.target\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "            output,val_h = model(inputs,val_h)\n",
    "            val_loss = criterion(output,targets.view(-1).long())\n",
    "            val_losses.append(val_loss.item())\n",
    "    print(f\"Train loss is {np.mean(train_losses)} val loss is {np.mean(val_losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None):\n",
    "        # tensor inputs\n",
    "        x = torch.tensor([[TEXT.vocab.stoi[char]]])\n",
    "        inputs = x.to(device)\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "        \n",
    "        p = out.squeeze().div(temperature).exp().cpu()\n",
    "\n",
    "        char = torch.multinomial(p,1)[0]\n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return TEXT.vocab.itos[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='पुलिस का कहना है'):\n",
    "    with torch.no_grad():    \n",
    "        net.eval() # eval mode\n",
    "        chars=[]\n",
    "        h = net.init_hidden(1)\n",
    "        for ch in sp.EncodeAsPieces(prime):\n",
    "            chars.append(ch)\n",
    "            char, h = predict(net, ch, h )\n",
    "        chars.append(char)\n",
    "            #     # Now pass in the previous word and get a new one\n",
    "        for ii in range(size):\n",
    "            char, h = predict(net, chars[-1], h)\n",
    "            chars.append(char)\n",
    "\n",
    "        return sp.decode_pieces(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"हमलावर नजदीकी समस्या जल्दी जाने 'अनजाने सुरक्षा विभाग में लगे हमलों का एक और्दा बताया गया है। बीबीसी के पूर्व मोंग्राअस्पताल में 2008 में अमरीकी विदेशमंत्री मिश्रण ने करीब जो दूरी हासिल कर दिया है। इन तीनों मीबैन (एनआईएलडो) के निदेशक हमें भ्रष्टाचार की संख्या तेजी से कोई कम बम हैं। इसदौरान आधिकारिन और इटहों में 1948किलोमीटर-मन में कुल भर जाएगा जब प्रोजेक्ट के परिसर में लिस्टेट वालीसॉटफों के तहत विफल करवाबॉ सके प्रकटकर दी जाएगी। पर अभीतक मोतियाबिंद का कहना है कि बेगओन ने शुक्रवार को 753 रनों में ऑस्ट्रेलिया एक इमारत बढ़ाने का अमेरिकी अपबोज कर लिया। हवाईअड्डे में श्रीलंका की टीम ने राज को सार्वजनिक करने और 1986 को बैठे बल्लेबाजियों पर रूस में पहले प्रदर्शन हासिल करने के बाद इसे बचाते हैं। उन्होंने बताया कि मेक्सिको के साथ बॉल बनाए हुए खंडड़ा सहित श्रीलंका का हिस्सा तिमाहियों से गिरफ्तार किया जाना होगा।\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model,200,prime=\"हमलावर नजदीकी\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
