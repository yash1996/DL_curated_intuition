{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model\n",
    "\n",
    "It is the development of probabilistic models that are able to predict the next word in the sequence given the words that precede it.\n",
    "\n",
    "Language models (LM) can be classiﬁed into two categories: \n",
    "1. Count-based \n",
    "2. Continuous-space LM or Neural Language Model(NLM)\n",
    "\n",
    "\n",
    "**1. Count based**\n",
    "<br>\n",
    " The count-based methods, such as traditional statistical models, usually involve making an n-th order Markov assumption and estimating n-gram probabilities via counting and subsequent smoothing.\n",
    " Eg:modiﬁed Kneser-Ney smoothing, Jelinek-Mercer smoothing\n",
    " \n",
    "Drawback:\n",
    "1. **Problem of sparsity.**\n",
    "   <br>\n",
    "   when modeling the joint distribution of a sentence, a simple n-gram model would give zero probability to all of the \n",
    "   combination that were not encountered in the training corpus, i.e. it would most likely give zero probability to most of \n",
    "   the out-of-sample test cases. However, new combinations of n words that were not seen in the training set are likely to \n",
    "   occur, thus we do not want to assign such cases zero probability. The traditional solution is to use various back-off  and \n",
    "   smoothing techniques, but no good solution exists.\n",
    "2. **Curse of Dimensionality**\n",
    "<br>\n",
    "    Huge number of different combinations of values of the input variables that must be discriminated from each other\n",
    "<br>\n",
    "3. **They rely on exact pattern,** i.e. string or word sequence matching, and therefore are in no way linguistically informed\n",
    "    “the cat is walking in the bedroom” to be syntactically and semantically similar to “a dog was running in the room”, which \n",
    "    cannot be provided by an n-gram model\n",
    "4.  **Dependency beyond the window is ignored**\n",
    "    Estimating the parameters of the n-gram model, we only consider context of n-1 words\n",
    "\n",
    "**2. Continuos-Space**\n",
    "<br>\n",
    "Continuous-space LM includes the feed-forward neural probabilistic language models (NPLMs) and recurrent neural network language models (RNNs).It solves the problem of data sparsity of the n-gram model,\n",
    "Different type of NML\n",
    "1. **Feed-Forward neural network based LM**  -  Proposed to tackle the problems of data sparsity\n",
    "2. **Recurrent neural network based LM**  -    Proposed to address the problem of limited context.\n",
    "\n",
    "\n",
    "1. **Feed-Forward neural network based LM**  \n",
    "<br>\n",
    "\n",
    "[ **Neural Probabilistic Language Model**](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "<br>\n",
    "It learns the parameters of conditional probability distribution of the next word, given the previous n-1 words using a feed-forward neural network of three layers.\n",
    "![](Images/neural_prob_model.PNG)\n",
    "\n",
    "\n",
    "[**Hierarchical Probabilistic Neural Network Language Model**]( https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf)\n",
    "<br>\n",
    "\n",
    "[**Hierarchical log-bilinear (HLBL) model**](https://www.cs.toronto.edu/~amnih/papers/hlbl_final.pdf)\n",
    "\n",
    "2. **Recurrent Neural Network Based Models**\n",
    "<br>\n",
    "\n",
    "[Recurrent Neural Network Based Models](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
    "<br>\n",
    "\n",
    "\n",
    "Unlike feed-forward neural network based LM which use fixed length context, recurrent neural network do not use limited size of context. By using recurrent connections, information cay cycle inside these networks for an arbitrary long time.\n",
    "The network architecture is as follows:\n",
    "\n",
    "    1. Input layer w and output layer y have the same dimensionality as the vocabulary (10K — 200K).\n",
    "    2. Hidden layer s is orders of magnitude smaller (50–1000 neurons).\n",
    "    3. U is the matrix of weights between input and hidden layer, \n",
    "    4. V is the matrix of weights between hidden and output layer.\n",
    "\n",
    "![](Images/rnn_lm.PNG)\n",
    "\n",
    "[Extension to RNNLM](http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf)\n",
    "\n",
    "This was proposed to further improve the original RNNLM by :\n",
    "1. Decreasing its computational complexity which was implemented by factorization of the output layer.\n",
    "2. Making the RNN model smaller and faster, both during training and testing, while being more accurate than the basic one\n",
    "![](Images/rnn_lm_ver2.PNG)\n",
    "\n",
    "\n",
    "[LSTM](https://www.aclweb.org/anthology/P16-1125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. https://mchromiak.github.io/articles/2017/Nov/30/Explaining-Neural-Language-Modeling/#.XQ-wXugzY2y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
