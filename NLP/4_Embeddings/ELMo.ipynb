{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ELMo (Embeddings from Language Model)](https://arxiv.org/pdf/1802.05365.pdf)\n",
    "\n",
    "The point where word2vec, Glove and fasttext failed is, The meaning, semantics of a words changes with respect to the words it is surrounded by in a sentence and these models wheren't able to represent that.\n",
    "\n",
    "For example:- \n",
    "\n",
    "1. Apple is in profit.\n",
    "2. Apple is tasty.\n",
    "\n",
    "Apple in first sentence refers to the company whereas apple in second sentence refers to fruit.\n",
    "\n",
    "In word2vec model this one word with two meaning will be represented by same vector.\n",
    "\n",
    "To solve this issue ELMo uses stacked bi-LSTM to generate embeddings for each words which is context dependent.\n",
    "\n",
    "so In the above example Apple in first and second case will have two different vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multi-layer Bi-LSTM take independent words as input and the hidden state of the LSTM is the output embedding representing the word.\n",
    "\n",
    "But Before passing the words as input to BI-LSTM the words are passed throught a char-CNN model to generate a fixed representation of a the word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Character-Aware Neural Language Models](https://arxiv.org/pdf/1508.06615.pdf)\n",
    "![](images/char_rnn.png)\n",
    "![](images/char_rnn_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using bidirectional LSTM so the forward and backward output of LSTM is returned as the embeddings for that particular word.\n",
    "\n",
    "output = [forward_representaion,backward_representation]\n",
    "\n",
    "we can average the two representation to get one representation of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But there is more to ELMo\n",
    "\n",
    "Since ELMo embeddings are trained in a unsupervised way. These embeddings can be used for downstream task in supervised way.\n",
    "\n",
    "To add ELMo in a supervised task. First Freeze the bi-LSTM layer and generate ELMo embeddings concatenate it with x and pass it to the the task RNN.\n",
    "\n",
    "output = [x,ELMo]\n",
    "\n",
    "where x is the normal context-independent token representation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
