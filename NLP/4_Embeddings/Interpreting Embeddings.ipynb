{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Embeddings to represent text into a numerical form. Either into a one-hot encoding format called sparse vector or a fixed Dense representation called Dense Vector.\n",
    "\n",
    "Every Word gets it meaning from the words it is surrounded by, So when we train our embeddings we want word with similar meaning or words used in similar context to be together.\n",
    "\n",
    "For Example:- \n",
    "1. Words like Aeroplane, chopper, Helicopter, Drone should be very close to each other because they share the same feature, they are flying object.\n",
    "\n",
    "2. Words like Man and Women should be exact opposite to each other.\n",
    "\n",
    "3. Sentences like \"Coders are boring people.\" and \"Programmers are boring.\" the word `coders` and `programmers` are used in similar context so they should be close to each other.\n",
    "\n",
    "Word Embeddings are nothing but vectors in a vector space. And using some vector calculation we can easily find \n",
    "1. Synonyms or similar words\n",
    "2. Finding Analogies\n",
    "3. Can be used as spell check (if trained on a large corpus)\n",
    "4. Pretty Much Anything which you can do with vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 400000 words in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "glove = torchtext.vocab.GloVe(name = '6B', dim = 100)\n",
    "\n",
    "print(f'There are {len(glove.itos)} words in the vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5450"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.stoi[\"cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word):\n",
    "    return glove.vectors[glove.stoi[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2309,  0.2828,  0.6318, -0.5941, -0.5860,  0.6326,  0.2440, -0.1411,\n",
       "         0.0608, -0.7898, -0.2910,  0.1429,  0.7227,  0.2043,  0.1407,  0.9876,\n",
       "         0.5253,  0.0975,  0.8822,  0.5122,  0.4020,  0.2117, -0.0131, -0.7162,\n",
       "         0.5539,  1.1452, -0.8804, -0.5022, -0.2281,  0.0239,  0.1072,  0.0837,\n",
       "         0.5501,  0.5848,  0.7582,  0.4571, -0.2800,  0.2522,  0.6896, -0.6097,\n",
       "         0.1958,  0.0442, -0.3114, -0.6883, -0.2272,  0.4618, -0.7716,  0.1021,\n",
       "         0.5564,  0.0674, -0.5721,  0.2374,  0.4717,  0.8277, -0.2926, -1.3422,\n",
       "        -0.0993,  0.2814,  0.4160,  0.1058,  0.6220,  0.8950, -0.2345,  0.5135,\n",
       "         0.9938,  1.1846, -0.1636,  0.2065,  0.7385,  0.2406, -0.9647,  0.1348,\n",
       "        -0.0072,  0.3302, -0.1236,  0.2719, -0.4095,  0.0219, -0.6069,  0.4076,\n",
       "         0.1957, -0.4180,  0.1864, -0.0327, -0.7857, -0.1385,  0.0440, -0.0844,\n",
       "         0.0491,  0.2410,  0.4527, -0.1868,  0.4618,  0.0891, -0.1819, -0.0152,\n",
       "        -0.7368, -0.1453,  0.1510, -0.7149])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similar Context\n",
    "\n",
    "To find words similar to input words. We have to first take the vector representation of all words and compute the eucledian distance of the input word with respect to all words and choose the n closest words by sorting the distance ascending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_word(word,n=10):\n",
    "    input_vector = get_embedding(word).numpy() if isinstance(word,str) else word.numpy()\n",
    "    distance = np.linalg.norm(input_vector-glove.vectors.numpy(),axis=1)\n",
    "    sort_dis = np.argsort(distance)[:n]\n",
    "    return list(zip(np.array(glove.itos)[sort_dis] , distance[sort_dis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sad', 0.0),\n",
       " ('sorry', 3.8980238),\n",
       " ('awful', 3.9397242),\n",
       " ('tragic', 4.1759458),\n",
       " ('horrible', 4.2391276),\n",
       " ('heartbreaking', 4.3083005),\n",
       " ('unfortunate', 4.3767824),\n",
       " ('pathetic', 4.384075),\n",
       " ('scary', 4.3990903),\n",
       " ('happy', 4.427562)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_word(\"sad\",n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_angle(word1,word2):\n",
    "    word1 = get_embedding(word1).view(1,-1)\n",
    "    word2 = get_embedding(word2).view(1,-1)\n",
    "    simi = torch.nn.CosineSimilarity(dim=1)(word1,word2).numpy()    \n",
    "    return simi,np.rad2deg(np.arccos(simi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.7284238], dtype=float32), array([43.245583], dtype=float32))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity_angle(\"sad\",\"awful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy( word1, word2, word3, n=5):\n",
    "    \n",
    "    #get vectors for each word\n",
    "    word1_vector = get_embedding(word1)\n",
    "    word2_vector = get_embedding(word2)\n",
    "    word3_vector = get_embedding(word3)\n",
    "    \n",
    "    #calculate analogy vector\n",
    "    analogy_vector = word2_vector - word1_vector + word3_vector\n",
    "    \n",
    "#     #find closest words to analogy vector\n",
    "    candidate_words = get_closest_word( analogy_vector, n=n+3)\n",
    "    \n",
    "    #filter out words already in analogy\n",
    "    candidate_words = [(word, dist) for (word, dist) in candidate_words \n",
    "                       if word not in [word1, word2, word3]][:n]\n",
    "    \n",
    "    print(f'{word1} is to {word2} as {word3} is to...')\n",
    "    \n",
    "    return candidate_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man is to king as woman is to...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('queen', 4.081079),\n",
       " ('monarch', 4.6429076),\n",
       " ('throne', 4.9055004),\n",
       " ('elizabeth', 4.921559),\n",
       " ('prince', 4.9811463)]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('man', 'king', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the canonical example which shows off this property of word embeddings. So why does it work? Why does the vector of 'woman' added to the vector of 'king' minus the vector of 'man' give us 'queen'?\n",
    "\n",
    "If we think about it, the vector calculated from 'king' minus 'man' gives us a \"royalty vector\". This is the vector associated with traveling from a man to his royal counterpart, a king. If we add this \"royality vector\" to 'woman', this should travel to her royal equivalent, which is a queen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india is to delhi as australia is to...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sydney', 3.075962),\n",
       " ('melbourne', 3.1802053),\n",
       " ('canberra', 3.2988422),\n",
       " ('perth', 3.3058352),\n",
       " ('brisbane', 3.543623)]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('india', 'delhi', 'australia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reliable', 0.0),\n",
       " ('dependable', 3.4597418),\n",
       " ('accurate', 3.9092937),\n",
       " ('unreliable', 4.1367426),\n",
       " ('trustworthy', 4.178639),\n",
       " ('consistent', 4.251687),\n",
       " ('useful', 4.2767124),\n",
       " ('efficient', 4.4032826),\n",
       " ('credible', 4.4201684),\n",
       " ('authoritative', 4.514892)]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_word(\"reliable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Studies\n",
    "1. https://forums.fast.ai/t/nlp-any-libraries-dictionaries-out-there-for-fixing-common-spelling-errors/16411\n",
    "\n",
    "2. Multilingual and Cross-lingual analysis: If you work on works in translation, or on the influence of writers who write in one language on those who write in another language, word vectors can valuable ways to study these kinds of cross-lingual relationships algorithmically.\n",
    "[Case Study: Using word vectors to study endangered languages](https://raw.githubusercontent.com/YaleDHLab/lab-workshops/master/word-vectors/papers/coeckelbergs.pdf)\n",
    "\n",
    "3. Studying Language Change over Time: If you want to study the way the meaning of a word has changed over time, word vectors provide an exceptional method for this kind of study.\n",
    "[Case Study: Using word vectors to analyze the changing meaning of the word \"gay\" in the twentieth century.](https://nlp.stanford.edu/projects/histwords/)\n",
    "\n",
    "4. Analyzing Historical Concept Formation: If you want to analyze the ways writers in a given historical period understood particular concepts like \"honor\" and \"chivalry\", then word vectors can provide excellent opportunities to uncover these hidden associations.\n",
    "[Case Study: Using word vectors to study the ways eighteenth-century authors organized moral abstractions](https://raw.githubusercontent.com/YaleDHLab/lab-workshops/master/word-vectors/papers/heuser.pdf)\n",
    "\n",
    "5. Uncovering Text Reuse: If you want to study text reuse or literary imitation (either within one language or across multiple languages), word vectors can provide excellent tools for identifying similar passages of text.\n",
    "[Case Study: Using word vectors to uncover cross-lingual text reuse in eighteenth-century writing](https://douglasduhaime.com/posts/crosslingual-plagiarism-detection.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
