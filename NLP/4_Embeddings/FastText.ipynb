{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "\n",
    "Fasttext Released two papers \n",
    "\n",
    "[**1. Bags of tricks for Efficient Text Classification**](https://arxiv.org/pdf/1607.01759.pdf)\n",
    "\n",
    "[**2. Enriching Word Vectors with Subword Information**](https://arxiv.org/pdf/1607.04606.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bags of Tricks for Efficient Text Classification\n",
    "\n",
    "Let's see what tricks they used\n",
    "1. Fairly Simple Model Architecture\n",
    "2. Hierarchical Softmax\n",
    "3. N-Gram Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "![](images/fasttext_Arch.png)\n",
    "\n",
    "All the input tokens for a document are passed to an Embedding layer after which they are averaged out to generate an embedding for the sentence.\n",
    "The Sentence embeddings is passed to a Fully Connected Softmax layer for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Softmax\n",
    "\n",
    "When the number of classes is large, computing the linear classifier is computationally expensive.  Moreprecisely,  the  computational  complexity  is O(kh) where k is  the  number  of  classes  and h the  dimension of the text representation.  In order to improve the running time, Hierarchical softmax is used droping the the computational complexity O(hlog2(k)).\n",
    "\n",
    "Normal Softmax\n",
    "![](images/softmax.png)\n",
    "\n",
    "Hierarchical Softmax\n",
    "![](images/h_softmax.png)\n",
    "\n",
    "In Hierarchical softmax the task is to form tree.\n",
    "\n",
    "Good Tree\n",
    "![](images/good_tree.png)\n",
    "\n",
    "Bad Tree\n",
    "![](images/bad_tree.png)\n",
    "\n",
    "Hierarchical softmax is not an approximate optimization algorithm. It accelerates the optimization by adding human orchestrations which could be highly biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Features\n",
    "In fasttext they incorporate n-grams with the actual tokens in text. This adds more feature regarding the local context of how the words are used together.\n",
    "\n",
    "More specifically they have used bi-gram.\n",
    "\n",
    "For example:-\n",
    "\n",
    "    text = I love my country\n",
    "\n",
    "    bi-gram feature = \"I\" , \"love\" , \"my\", \"country\" , \"I love\" , \"love my\" , \"my country\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriching Word Vectors with Subword Information\n",
    "\n",
    "Continuous  word  representations,  trained  on large  unlabeled  corpora  are  useful  for  many natural  language  processing  tasks.    Popular models that learn such representations ignore the morphology of words, by assigning a dis-tinct vector to each word. This is a limitation,especially for languages with large vocabularies and many rare words.\n",
    "\n",
    "Considering Unigram word model they are higher chances of getting rare words during testing because we can't take infinite size corpous which includes all the words.\n",
    "\n",
    "Here each word is represented as a bag of character n-grams. A vector representation of each character n-gram is summed to get word vector.\n",
    "\n",
    "By representing each word as a bag of character n-grams a SkipGram model with Negative sampling is trained.\n",
    "\n",
    "By  using  a  distinct  vector  representation  for  each word, the skipgram model ignores the internal structure of words.\n",
    "\n",
    "![](images/subword_1.png)\n",
    "![](images/subword_2.png)\n",
    "\n",
    "Hyperparameter choice for generating Fasttext embeddings\n",
    "\n",
    "1. Generating fasttext embedding will take more time compared to word2vec model.\n",
    "2. As the corpus size grows, the memory requirement grows too - the number of ngrams that get hashed into the same ngram bucket would grow. So the choice of hyperparameter controlling the total hash buckets including the n-gram min and max size have a bearing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
